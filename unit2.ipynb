{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unit2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDoZjHNH/vosRvrV3q4Jsx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ujin2021/ML_CrashCourse/blob/main/unit2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_3ZU3qPWeFi"
      },
      "source": [
        "## How to reduce loss?\n",
        "Model) y' = w₁x₁+ b <br>\n",
        "How to initialize w₁ and b? => pick random value\n",
        "\n",
        "* b = 0, w₁ = 0\n",
        "\n",
        "### Squared Loss Function\n",
        "* y' : The model's prediction for features x\n",
        "* y : The correct label corresponding to features x\n",
        "* 손실함수 값을 계산 후, b와 w₁값을 업데이트 한다\n",
        "* 손실 가능성이 가장 낮은 모델 매개 변수를 찾을 때까지(수렴, convergence) 학습이 계속 반복\n",
        "\n",
        "### Gradient Descent(경사 하강법)\n",
        "* 경사 하강 법 알고리즘은 시작점에서 손실 곡선의 경사(gradient)를 계산\n",
        "* gradient is a ***vector***\n",
        "  * a direction\n",
        "  * a magniture\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/53362054/100301276-2d57bc80-2fdb-11eb-9cc9-9b7ab717eda0.png\" width=300 height=190>\n",
        "<img src=\"https://user-images.githubusercontent.com/53362054/100301337-50826c00-2fdb-11eb-9370-96c918ddd636.png\" width=300 height=190>\n",
        "\n",
        "* 초기값에서의 경사(gradient)를 계산\n",
        "* gradient의 최소값은 0(minimum)\n",
        "* 계산한 값이 음수(기울기가 음수) -> next point를 오른쪽으로 옮긴다\n",
        "* 계산한 값이 양수(기울기가 양수) -> next point를 왼쪽으로 옮긴다\n",
        "* b, w₁값을 바꿔가면서 최소값과 같아질 때 까지 계산을 반복\n",
        "\n",
        "### Learning Rate\n",
        "* 다음 지점을 정할 때 Learning rate(step size)를 사용한다\n",
        "* Learning rate가 너무 작다 -> 학습의 시간이 오래걸린다\n",
        "* Learning rate가 너무 크다 -> 왔다갔다 폭이 너무 크다\n",
        "* Learning rate가 적절하다(Goldilocks learning rate) -> 최소 손실을 달성하기 위해 가능한 가장 적은 단계를 수행하는 학습률\n",
        "\n",
        "### Batch\n",
        "* batch : 단일반복에서 경사를 계산하는데 사용하는 총 예제수(The set of examples used in one iteration (that is, one gradient update) of model training.)\n",
        "* batch는 모든 데이터를 학습하는데 활용하므로 부드럽게 수렴하지만 시간이 오래걸림\n",
        "* batch  size : The number of examples in a batch\n",
        "\n",
        "### Stochastic Gradient Descent(SGD, 확률적 경사 하강법)\n",
        "* 전체 데이터를 사용하는 것이 아닌 random으로 예제를 뽑음(batch size = 1)\n",
        "* 진폭은 크지만, 시간을 빠르다\n",
        "\n",
        "### Mini-batch stochastic gradient descent (mini-batch SGD)\n",
        "* full- batch와 SGD의 절충안이다\n",
        "* SGD의 noise를 줄이고, full-batch보다 효율적이다\n",
        "* batch size : 10 and 1000\n",
        "\n",
        "* large data set에서는 batch size를 full-batch 보다 SGD를 사용하는 것이 효율적이다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvfwJRBf9wy_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}